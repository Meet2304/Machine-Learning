{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a275d54a",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f1f941",
   "metadata": {},
   "source": [
    "- Feature selection is the process of choosing the most relevant features (input variables) from your dataset that contribute the most to predicting the target variable.\n",
    "- We remove irrelevant/redundant features\n",
    "- It’s different from feature extraction (like PCA), which creates new transformed features rather than selecting existing ones.\n",
    "\n",
    "### Why its done:\n",
    "- It is implemented to reduce overfitting\n",
    "- It improves the training speed and increases accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e66f05",
   "metadata": {},
   "source": [
    "# Types of Feature Selection Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6849ba40",
   "metadata": {},
   "source": [
    "## 1) Filter Methods\n",
    "- They work by applying statistical tests to measure the corerlation or relevance of each feature to the target variable\n",
    "- Features are ranked and selected based on scores like correlation, mutual information, chi-square, etc\n",
    "- This works well as a pre-processing step before modelling\n",
    "- This method ignores considering each feature individually on its own merit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8a81cd",
   "metadata": {},
   "source": [
    "## 2) Wrapper Methods\n",
    "- Treats feature selection as a search problem\n",
    "- We train different models on different subsets of the data and evaluate the model performance for each subset\n",
    "- The goal is to find the optimal subset of features that gives the best preditive performance\n",
    "- This is computationally very expensive and may overfit on small datasets\n",
    "\n",
    "### Common Techniques:\n",
    "- Forward Selection → start with no features, add one at a time\n",
    "- Backward Elimination → start with all features, remove one at a time\n",
    "- Recursive Feature Elimination (RFE) → iteratively train a model, remove least important features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71534af4",
   "metadata": {},
   "source": [
    "## 3) Embedded Models\n",
    "- Feature selection takes place during model training\n",
    "- The model learns which features are most important using weights, regularization or built in feature importance\n",
    "\n",
    "### Common Techniques\n",
    "- Lasso Regression (L1 regularization) → shrinks some coefficients to zero, effectively removing them\n",
    "- Ridge Regression (L2 regularization) → reduces weight but doesn’t remove features\n",
    "- Decision Trees / Random Forest / XGBoost feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ab0363",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
